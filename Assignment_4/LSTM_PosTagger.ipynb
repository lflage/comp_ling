{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do\n",
    "\n",
    "[X] Load the data  \n",
    "[ ] Design the LSTM model  \n",
    "  \n",
    "    [X] Embedding layer. \n",
    "    [X] Linear Layer  \n",
    "[ X ] Train function  \n",
    "   \n",
    "    [X] Cross Entropy loss  \n",
    "    [X] Adam Optmizer  \n",
    "    \n",
    "[__] COMET\n",
    "\n",
    "\n",
    "#### Evaluation\n",
    "[  ] Tagging accuracy on a given sentence\n",
    "\n",
    "[  ] Accuracy on __Development__ Corpus after __each__ epoch\n",
    "\n",
    "[  ] Accuracy on __Trainning__ Corpus after __each__ epoch\n",
    "\n",
    "[  ] Accuracy on __Test__ Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing comet first\n",
    "from comet_ml import Experiment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import data\n",
    "\n",
    "# Importing comet library for accuracy evaluation\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "##\n",
    "# Loading the corpus into the following variables\n",
    "#    train_dataloader           DataLoader for iterating over the training data\n",
    "#    dev_dataloader             DataLoader for iterating over the development data\n",
    "#    test_dataloader            DataLoader for iterating over the test data\n",
    "#    vocabulary                 Vocabulary of words in the sentences in the data\n",
    "#    tagset                     Vocabulary of POS tags in the data\n",
    "#    pretrained_embeddings      Pretrained fasttext word embeddings \n",
    "##\n",
    "\n",
    "train_dataloader,dev_dataloader,test_dataloader,vocabulary,tagset,pretrained_embeddings = data.load(\n",
    "'corpus/de_gsd-ud-train.conllu',\n",
    "'corpus/de_gsd-ud-dev.conllu',\n",
    "'corpus/de_gsd-ud-test.conllu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Print information of Embeddings, Tagset and Vocab\n",
    "##\n",
    "\n",
    "print(\"\\nPretrained Embeddings shape:\")\n",
    "print(pretrained_embeddings.shape)\n",
    "\n",
    "print(\"\\nTagset size:{}\\n\".format(len(tagset)))\n",
    "print('Tags:\\n{}'.format(tagset.lookup_tokens(range(0,len(tagset)))))\n",
    "\n",
    "print(\"\\nVocab size: {}\".format(len(vocabulary)))\n",
    "\n",
    "print(\"\\nVocab sample:\")\n",
    "print(vocabulary.lookup_tokens(range(0,50)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for minibatch in train_dataloader:\n",
    "\n",
    "    print('Shape of the batch Tensor objects: {}\\n'.format(minibatch.size()))\n",
    "    print('\\nFirst POS Tags:\\n')\n",
    "    print(' '.join(tagset.lookup_tokens(minibatch[0][1].flatten().tolist())))\n",
    "    print('\\nFirst Sentence:\\n')\n",
    "    print(' '.join(vocabulary.lookup_tokens(minibatch[0][0].flatten().tolist())))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPosTagger(nn.Module):\n",
    "    def __init__(self,\n",
    "        embeddings,\n",
    "        hidden_dim,\n",
    "        tagset_size):\n",
    "        super(LSTMPosTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "\n",
    "        # The LSTM receives the word embedding vector a input and outputs\n",
    "        # a vector of size int(hidden_dim). This size can be changed to test\n",
    "        # it's influence on the model performance\n",
    "        self.lstm = nn.LSTM(self.word_embeddings.embedding_dim, hidden_dim)\n",
    "\n",
    "        # A Linear layer that receives the output of the LSTM model with\n",
    "        # size int(hidden_dim) and outputs a vector of size int(tagset_size) \n",
    "        self.hid_to_tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        '''sentence is a list of indices for the words in the pre trained embedding\n",
    "        model. Embeds '''\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        ''''''\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_scores = self.hid_to_tag(lstm_out.view(len(sentence), -1))\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: Module, \n",
    "                train_data: DataLoader,\n",
    "                num_epochs: int,\n",
    "                optimizer_type,\n",
    "                loss_function,\n",
    "                learning_rate: float) -> None:\n",
    "    \"\"\"\n",
    "    runs one commplete training run, i.e. trains the model on your training data for\n",
    "    :param model: a pytorch model\n",
    "    :param train_data: a dataloader for getting the training instances\n",
    "    :param num_epochs: the number of epochs to train\n",
    "    :param optimizer_type: the type of optimizer to use for training\n",
    "    :param loss_function: the type of loss function to use\n",
    "    :param learning_rate: the learning rate for the optimizer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print(f'--------- Start Training ------------')\n",
    "\n",
    "    # Important: bring model into training mode\n",
    "    model.train()\n",
    "    experiment = Experiment()\n",
    "    \n",
    "    optimizer = optimizer_type(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # run training for specified number of epochs; use tqdm to keep track of progress / estimated run time \n",
    "    #with experiment.train():\n",
    "    for epoch in tqdm(range(num_epochs), desc='Classifier Training\\n'):\n",
    "        cum_loss = 0\n",
    "        print(f'---------- Started Epoch {epoch} -----------')\n",
    "    \n",
    "        for batch in train_data:\n",
    "            # get the input instances (and move them to the device you use)\n",
    "            input_attributes = batch[0][0].to(device)\n",
    "            # get the corresponding labels\n",
    "            gold_labels = batch[0][1].to(device)\n",
    "    \n",
    "    \n",
    "            # compute model predictions with current model parameters\n",
    "            model_output = model(input_attributes)\n",
    "    \n",
    "            # Compute Loss for current batch\n",
    "            loss = loss_function(model_output, gold_labels)\n",
    "            cum_loss += loss.item()\n",
    "                \n",
    "                \n",
    "                #Important: otherwise you add up your gradients for all batches and for all epochs\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            loss.backward()\n",
    "    \n",
    "                # Update parameters\n",
    "            optimizer.step()\n",
    "                ##################################################\n",
    "#                 # comet stuff\n",
    "#                 # Compute train accuracy\n",
    "#                 _, predicted = torch.max(model_output.data, 1)\n",
    "#                 batch_total = labels.size(0)\n",
    "#                 total += batch_total\n",
    "\n",
    "#                 batch_correct = (predicted == labels.data).sum()\n",
    "#                 correct += batch_correct\n",
    "\n",
    "#                 # Log batch_accuracy to Comet.ml; step is each batch\n",
    "#                 step += 1\n",
    "#                 experiment.log_metric(\"batch_accuracy\", batch_correct / batch_total, step=step)\n",
    "                ################################################\n",
    "            \n",
    "        mean_loss_per_epoch = cum_loss/len(train_data)\n",
    "        print(mean_loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_comet(model: Module, \n",
    "                train_data: DataLoader,\n",
    "                num_epochs: int,\n",
    "                optimizer_type,\n",
    "                loss_function,\n",
    "                learning_rate: float) -> None:\n",
    "    \"\"\"\n",
    "    runs one commplete training run, i.e. trains the model on your training data for\n",
    "    :param model: a pytorch model\n",
    "    :param train_data: a dataloader for getting the training instances\n",
    "    :param num_epochs: the number of epochs to train\n",
    "    :param optimizer_type: the type of optimizer to use for training\n",
    "    :param loss_function: the type of loss function to use\n",
    "    :param learning_rate: the learning rate for the optimizer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print(f'--------- Start Training ------------')\n",
    "\n",
    "    # Important: bring model into training mode\n",
    "    model.train()\n",
    "    experiment = Experiment()\n",
    "    \n",
    "    optimizer = optimizer_type(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # run training for specified number of epochs; use tqdm to keep track of progress / estimated run time \n",
    "    with experiment.train():\n",
    "        step=0\n",
    "\n",
    "        for epoch in tqdm(range(num_epochs), desc='Classifier Training\\n'):\n",
    "            # Cummulative loss per batch\n",
    "            cum_loss = 0\n",
    "            # Number of correct predictions\n",
    "            correct = 0\n",
    "            # Number of total tokens predicted\n",
    "            total = 0\n",
    "            \n",
    "            print(f'---------- Started Epoch {epoch} -----------')\n",
    "\n",
    "            for batch in train_data:\n",
    "                # get the input instances (and move them to the device you use)\n",
    "                input_attributes = batch[0][0].to(device)\n",
    "                # get the corresponding labels\n",
    "                gold_labels = batch[0][1].to(device)\n",
    "                \n",
    "                # compute model predictions with current model parameters\n",
    "                model_output = model(input_attributes)\n",
    "    \n",
    "                # Compute Loss for current batch\n",
    "                loss = loss_function(model_output, gold_labels)\n",
    "                cum_loss += loss.item()\n",
    "                  \n",
    "                #Important: otherwise you add up your gradients for all batches and for all epochs\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                loss.backward()\n",
    "    \n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "            \n",
    "                ##################################################\n",
    "                 # comet stuff\n",
    "                 # Compute train accuracy\n",
    "                # Torch.max returns a namedtuple where of (value,indices)\n",
    "                # where ```values``` is the maximum value of each rou of the\n",
    "                # input tensor in the given dimension \n",
    "                _, predicted = torch.max(model_output.data, 1)\n",
    "\n",
    "                batch_total = gold_labels.size(0)\n",
    "                total += batch_total\n",
    "\n",
    "                batch_correct = (predicted == gold_labels.data).sum()\n",
    "                correct += batch_correct\n",
    "\n",
    "                 # Log batch_accuracy to Comet.ml; step is each batch\n",
    "            step+=1\n",
    "            experiment.log_metric(\"mean_epoch_accuracy\", batch_correct / batch_total, step=step)\n",
    "                ################################################\n",
    "                \n",
    "            mean_loss_per_epoch = cum_loss/len(train_data)\n",
    "            experiment.log_metric('Mean_loss_per_epoch',mean_loss_per_epoch,step)\n",
    "            print(mean_loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 300\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "EMBEDDINGS = pretrained_embeddings\n",
    "\n",
    "OPTIMIZER = optim.Adam\n",
    "\n",
    "LOSS_FUNCTION = nn.functional.cross_entropy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TAGGER = LSTMPosTagger(pretrained_embeddings,HIDDEN_SIZE,len(tagset))\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Start Training ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/lflage/general/dc678f7a701a4f0d984ac123b72dbb07\n",
      "\n",
      "Classifier Training\n",
      ":   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Started Epoch 0 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":   1%|          | 1/100 [00:05<09:49,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6146562993922766\n",
      "---------- Started Epoch 1 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":   2%|▏         | 2/100 [00:10<09:03,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2712990739715002\n",
      "---------- Started Epoch 2 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":   3%|▎         | 3/100 [00:15<08:37,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2106187020754704\n",
      "---------- Started Epoch 3 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":   4%|▍         | 4/100 [00:19<08:08,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1722192650528503\n",
      "---------- Started Epoch 4 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":   5%|▌         | 5/100 [00:24<07:46,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18616029434357753\n",
      "---------- Started Epoch 5 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":   6%|▌         | 6/100 [00:28<07:30,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17654213385508005\n",
      "---------- Started Epoch 6 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":   7%|▋         | 7/100 [00:33<07:19,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15248386463545888\n",
      "---------- Started Epoch 7 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":   8%|▊         | 8/100 [00:38<07:12,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1329423670939512\n",
      "---------- Started Epoch 8 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":   9%|▉         | 9/100 [00:42<07:04,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12755172424842615\n",
      "---------- Started Epoch 9 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  10%|█         | 10/100 [00:47<06:58,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12044586720604754\n",
      "---------- Started Epoch 10 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  11%|█         | 11/100 [00:51<06:52,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12649930557455355\n",
      "---------- Started Epoch 11 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  12%|█▏        | 12/100 [00:56<06:47,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13038252236974165\n",
      "---------- Started Epoch 12 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  13%|█▎        | 13/100 [01:01<06:41,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1193257122388398\n",
      "---------- Started Epoch 13 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  14%|█▍        | 14/100 [01:05<06:36,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10024681747879097\n",
      "---------- Started Epoch 14 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  15%|█▌        | 15/100 [01:10<06:31,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09589408039357934\n",
      "---------- Started Epoch 15 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  16%|█▌        | 16/100 [01:14<06:27,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08873454130269116\n",
      "---------- Started Epoch 16 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  17%|█▋        | 17/100 [01:19<06:24,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09484581732923207\n",
      "---------- Started Epoch 17 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  18%|█▊        | 18/100 [01:24<06:21,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1217892398616361\n",
      "---------- Started Epoch 18 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  19%|█▉        | 19/100 [01:29<06:19,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11826738785761295\n",
      "---------- Started Epoch 19 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  20%|██        | 20/100 [01:33<06:16,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09688493358757096\n",
      "---------- Started Epoch 20 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  21%|██        | 21/100 [01:38<06:13,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08539769141407647\n",
      "---------- Started Epoch 21 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  22%|██▏       | 22/100 [01:43<06:11,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07609470811798126\n",
      "---------- Started Epoch 22 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  23%|██▎       | 23/100 [01:48<06:09,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06963898114986107\n",
      "---------- Started Epoch 23 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  24%|██▍       | 24/100 [01:53<06:07,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06440856564737676\n",
      "---------- Started Epoch 24 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  25%|██▌       | 25/100 [01:58<06:04,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0970813098916114\n",
      "---------- Started Epoch 25 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  26%|██▌       | 26/100 [02:03<06:06,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11216897273738789\n",
      "---------- Started Epoch 26 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  27%|██▋       | 27/100 [02:08<06:05,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09354398460694018\n",
      "---------- Started Epoch 27 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  28%|██▊       | 28/100 [02:13<06:03,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07486918068656462\n",
      "---------- Started Epoch 28 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  29%|██▉       | 29/100 [02:18<06:03,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06263040010022218\n",
      "---------- Started Epoch 29 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  30%|███       | 30/100 [02:24<06:02,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06831945454198832\n",
      "---------- Started Epoch 30 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  31%|███       | 31/100 [02:29<06:01,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06162892401251119\n",
      "---------- Started Epoch 31 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  32%|███▏      | 32/100 [02:35<06:00,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06011142423709556\n",
      "---------- Started Epoch 32 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  33%|███▎      | 33/100 [02:40<05:59,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07007359662184864\n",
      "---------- Started Epoch 33 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  34%|███▍      | 34/100 [02:46<05:56,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07812357198418818\n",
      "---------- Started Epoch 34 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  35%|███▌      | 35/100 [02:51<05:54,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07702065324040885\n",
      "---------- Started Epoch 35 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  36%|███▌      | 36/100 [02:57<05:52,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059073418778518516\n",
      "---------- Started Epoch 36 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  37%|███▋      | 37/100 [03:02<05:49,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05677074190363136\n",
      "---------- Started Epoch 37 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  38%|███▊      | 38/100 [03:08<05:46,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.061934679364143985\n",
      "---------- Started Epoch 38 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  39%|███▉      | 39/100 [03:14<05:44,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06359140326927627\n",
      "---------- Started Epoch 39 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  40%|████      | 40/100 [03:20<05:42,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06998246989622872\n",
      "---------- Started Epoch 40 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  41%|████      | 41/100 [03:26<05:39,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.060610953855476525\n",
      "---------- Started Epoch 41 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  42%|████▏     | 42/100 [03:31<05:35,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.060828619681880955\n",
      "---------- Started Epoch 42 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  43%|████▎     | 43/100 [03:37<05:30,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06231081479733603\n",
      "---------- Started Epoch 43 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  44%|████▍     | 44/100 [03:43<05:27,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06566219072998751\n",
      "---------- Started Epoch 44 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  45%|████▌     | 45/100 [03:49<05:25,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0474411441258418\n",
      "---------- Started Epoch 45 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  46%|████▌     | 46/100 [03:55<05:21,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.037369463514121716\n",
      "---------- Started Epoch 46 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  47%|████▋     | 47/100 [04:01<05:17,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03526831303216849\n",
      "---------- Started Epoch 47 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  48%|████▊     | 48/100 [04:07<05:12,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052725575511898645\n",
      "---------- Started Epoch 48 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  49%|████▉     | 49/100 [04:14<05:08,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05185044086812938\n",
      "---------- Started Epoch 49 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  50%|█████     | 50/100 [04:20<05:03,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05170975478913193\n",
      "---------- Started Epoch 50 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  51%|█████     | 51/100 [04:26<04:59,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.051926224992155205\n",
      "---------- Started Epoch 51 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  52%|█████▏    | 52/100 [04:32<04:54,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07086003176690552\n",
      "---------- Started Epoch 52 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  53%|█████▎    | 53/100 [04:38<04:49,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0772071377020896\n",
      "---------- Started Epoch 53 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  54%|█████▍    | 54/100 [04:45<04:45,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07715599457345229\n",
      "---------- Started Epoch 54 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  55%|█████▌    | 55/100 [04:51<04:41,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07285270140185779\n",
      "---------- Started Epoch 55 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  56%|█████▌    | 56/100 [04:57<04:36,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07345095767796393\n",
      "---------- Started Epoch 56 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  57%|█████▋    | 57/100 [05:04<04:33,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08932095491510253\n",
      "---------- Started Epoch 57 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  58%|█████▊    | 58/100 [05:10<04:29,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05015966257964689\n",
      "---------- Started Epoch 58 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  59%|█████▉    | 59/100 [05:17<04:24,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04347881227333015\n",
      "---------- Started Epoch 59 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  60%|██████    | 60/100 [05:24<04:19,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04998395274636985\n",
      "---------- Started Epoch 60 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  61%|██████    | 61/100 [05:30<04:13,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04113539987016149\n",
      "---------- Started Epoch 61 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  62%|██████▏   | 62/100 [05:37<04:07,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.058062967865948875\n",
      "---------- Started Epoch 62 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  63%|██████▎   | 63/100 [05:43<04:01,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0821776148281746\n",
      "---------- Started Epoch 63 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  64%|██████▍   | 64/100 [05:50<03:55,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08855566208125355\n",
      "---------- Started Epoch 64 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  65%|██████▌   | 65/100 [05:56<03:50,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06396020351418909\n",
      "---------- Started Epoch 65 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  66%|██████▌   | 66/100 [06:03<03:44,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05521624253222553\n",
      "---------- Started Epoch 66 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  67%|██████▋   | 67/100 [06:10<03:39,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059530090545928165\n",
      "---------- Started Epoch 67 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  68%|██████▊   | 68/100 [06:17<03:32,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052060193450813114\n",
      "---------- Started Epoch 68 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  69%|██████▉   | 69/100 [06:23<03:26,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05464710588560909\n",
      "---------- Started Epoch 69 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  70%|███████   | 70/100 [06:30<03:19,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05526113377086732\n",
      "---------- Started Epoch 70 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  71%|███████   | 71/100 [06:37<03:13,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06611569845004465\n",
      "---------- Started Epoch 71 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  72%|███████▏  | 72/100 [06:43<03:07,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.055092401344532124\n",
      "---------- Started Epoch 72 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  73%|███████▎  | 73/100 [06:50<03:01,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.054539100381454396\n",
      "---------- Started Epoch 73 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  74%|███████▍  | 74/100 [06:57<02:55,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05621634755030862\n",
      "---------- Started Epoch 74 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  75%|███████▌  | 75/100 [07:04<02:52,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.055946502142307994\n",
      "---------- Started Epoch 75 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  76%|███████▌  | 76/100 [07:11<02:45,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04149049908886657\n",
      "---------- Started Epoch 76 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  77%|███████▋  | 77/100 [07:18<02:38,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.057483045627935145\n",
      "---------- Started Epoch 77 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  78%|███████▊  | 78/100 [07:25<02:31,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05243716829261243\n",
      "---------- Started Epoch 78 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  79%|███████▉  | 79/100 [07:32<02:24,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05368705654481023\n",
      "---------- Started Epoch 79 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  80%|████████  | 80/100 [07:39<02:18,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05530585472525115\n",
      "---------- Started Epoch 80 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  81%|████████  | 81/100 [07:46<02:11,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.062110336476792025\n",
      "---------- Started Epoch 81 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  82%|████████▏ | 82/100 [07:53<02:04,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05560632134733598\n",
      "---------- Started Epoch 82 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  83%|████████▎ | 83/100 [08:00<01:58,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0483118422410015\n",
      "---------- Started Epoch 83 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  84%|████████▍ | 84/100 [08:07<01:51,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056566945705750125\n",
      "---------- Started Epoch 84 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  85%|████████▌ | 85/100 [08:14<01:44,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053476853006718915\n",
      "---------- Started Epoch 85 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  86%|████████▌ | 86/100 [08:21<01:38,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04137146185847127\n",
      "---------- Started Epoch 86 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  87%|████████▋ | 87/100 [08:28<01:31,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046720912038352776\n",
      "---------- Started Epoch 87 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  88%|████████▊ | 88/100 [08:35<01:24,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0536643448827085\n",
      "---------- Started Epoch 88 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  89%|████████▉ | 89/100 [08:42<01:17,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05391095429895111\n",
      "---------- Started Epoch 89 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  90%|█████████ | 90/100 [08:49<01:10,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06423346059549079\n",
      "---------- Started Epoch 90 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  91%|█████████ | 91/100 [08:56<01:03,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05790895447632603\n",
      "---------- Started Epoch 91 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  92%|█████████▏| 92/100 [09:03<00:56,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05720074358782995\n",
      "---------- Started Epoch 92 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  93%|█████████▎| 93/100 [09:10<00:49,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04751562258325603\n",
      "---------- Started Epoch 93 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  94%|█████████▍| 94/100 [09:18<00:42,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04050406061272231\n",
      "---------- Started Epoch 94 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  95%|█████████▌| 95/100 [09:25<00:35,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03355187417814838\n",
      "---------- Started Epoch 95 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  96%|█████████▌| 96/100 [09:32<00:28,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0336011083147448\n",
      "---------- Started Epoch 96 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  97%|█████████▋| 97/100 [09:39<00:21,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0424794157735479\n",
      "---------- Started Epoch 97 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  98%|█████████▊| 98/100 [09:46<00:14,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059918900941769986\n",
      "---------- Started Epoch 98 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Classifier Training\n",
      ":  99%|█████████▉| 99/100 [09:53<00:07,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05805895871234694\n",
      "---------- Started Epoch 99 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Training\n",
      "Classifier Training100/100 [10:01<00:00,  7.21s/it]\n",
      ": 100%|██████████| 100/100 [10:01<00:00,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05336681526886888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model_comet(POS_TAGGER,\n",
    "            dev_dataloader,\n",
    "            NUM_EPOCHS,\n",
    "            OPTIMIZER,\n",
    "            LOSS_FUNCTION,\n",
    "            LEARNING_RATE)\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    # iterate over each label and check\n",
    "    for true, predicted in zip(y_true, y_pred):\n",
    "        if true == predicted:\n",
    "            correct_predictions += 1\n",
    "    # compute the accuracy\n",
    "    accuracy = correct_predictions/len(y_true)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TAGGER.eval()\n",
    "\n",
    "POS_TAGGER('')\n",
    "\n",
    "with experiment.test():\n",
    "    with torch.no_grad():   # important: otherwise you will compute gradients while running the model on your test data\n",
    "        correct=0\n",
    "        total=0\n",
    "           \n",
    "        test_predictions = None\n",
    "        test_targets = None\n",
    "        \n",
    "        for batch in test_dataloader:\n",
    "            print(f'--------- Evaluate Model ------------')\n",
    "            input_attributes = batch[0][0].to(device)\n",
    "            # get the corresponding labels\n",
    "            gold_labels = batch[0][1].to(device)\n",
    "\n",
    "            model_output = POS_TAGGER(input_attributes)\n",
    "            \n",
    "            print(gold_labels.size())\n",
    "            break\n",
    "\n",
    "        # run trained model on test instances\n",
    "\n",
    "        # compute evaluation metrics to evaluate the model performance based on predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TAGGER.eval()\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input_attributes = batch[0][0].to(device)\n",
    "    gold_labels = batch[0][1].to(device)\n",
    "    \n",
    "    model_output = POS_TAGGER(input_attributes)\n",
    "    \n",
    "    print(model_output.data)\n",
    "    print('\\n\\n\\nPREDICTED')\n",
    "    _, predicted = torch.max(model_output.data, 1)\n",
    "    print(predicted)\n",
    "    print( '\\n\\n\\nGOLD LABEL SIZE:')\n",
    "    print(gold_labels.size())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
