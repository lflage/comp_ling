{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do\n",
    "\n",
    "[X] Load the data  \n",
    "[ ] Design the LSTM model  \n",
    "  \n",
    "    [X] Embedding layer. \n",
    "    [X] Linear Layer  \n",
    "[ X ] Train function  \n",
    "   \n",
    "    [X] Cross Entropy loss  \n",
    "    [X] Adam Optmizer  \n",
    "    \n",
    "[__] COMET\n",
    "\n",
    "\n",
    "#### Evaluation\n",
    "[  ] Tagging accuracy on a given sentence\n",
    "\n",
    "[  ] Accuracy on __Development__ Corpus after __each__ epoch\n",
    "\n",
    "[  ] Accuracy on __Trainning__ Corpus after __each__ epoch\n",
    "\n",
    "[  ] Accuracy on __Test__ Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing comet first\n",
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "##\n",
    "# Loading the corpus into the following variables\n",
    "#    train_dataloader           DataLoader for iterating over the training data\n",
    "#    dev_dataloader             DataLoader for iterating over the development data\n",
    "#    test_dataloader            DataLoader for iterating over the test data\n",
    "#    vocabulary                 Vocabulary of words in the sentences in the data\n",
    "#    tagset                     Vocabulary of POS tags in the data\n",
    "#    pretrained_embeddings      Pretrained fasttext word embeddings \n",
    "##\n",
    "\n",
    "train_dataloader,dev_dataloader,test_dataloader,vocabulary,tagset,pretrained_embeddings = data.load(\n",
    "'corpus/de_gsd-ud-train.conllu',\n",
    "'corpus/de_gsd-ud-dev.conllu',\n",
    "'corpus/de_gsd-ud-test.conllu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Print information of Embeddings, Tagset and Vocab\n",
    "##\n",
    "\n",
    "print(\"\\nPretrained Embeddings shape:\")\n",
    "print(pretrained_embeddings.shape)\n",
    "\n",
    "print(\"\\nTagset size:{}\\n\".format(len(tagset)))\n",
    "print('Tags:\\n{}'.format(tagset.lookup_tokens(range(0,len(tagset)))))\n",
    "\n",
    "print(\"\\nVocab size: {}\".format(len(vocabulary)))\n",
    "\n",
    "print(\"\\nVocab sample:\")\n",
    "print(vocabulary.lookup_tokens(range(0,50)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for minibatch in train_dataloader:\n",
    "\n",
    "    print('Shape of the batch Tensor objects: {}\\n'.format(minibatch.size()))\n",
    "    print('\\nFirst POS Tags:\\n')\n",
    "    print(' '.join(tagset.lookup_tokens(minibatch[0][1].flatten().tolist())))\n",
    "    print('\\nFirst Sentence:\\n')\n",
    "    print(' '.join(vocabulary.lookup_tokens(minibatch[0][0].flatten().tolist())))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPosTagger(nn.Module):\n",
    "    def __init__(self,\n",
    "        embeddings,\n",
    "        hidden_dim,\n",
    "        tagset_size):\n",
    "        super(LSTMPosTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "\n",
    "        # The LSTM receives the word embedding vector a input and outputs\n",
    "        # a vector of size int(hidden_dim). This size can be changed to test\n",
    "        # it's influence on the model performance\n",
    "        self.lstm = nn.LSTM(self.word_embeddings.embedding_dim, self.hidden_dim)\n",
    "\n",
    "        # A Linear layer that receives the output of the LSTM model with\n",
    "        # size int(hidden_dim) and outputs a vector of size int(tagset_size) \n",
    "        self.hid_to_tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        '''sentence is a list of indices for the words in the pre trained embedding\n",
    "        model. Embeds '''\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        ''''''\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_scores = self.hid_to_tag(lstm_out.view(len(sentence), -1))\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_comet(model: Module, \n",
    "                train_data: DataLoader,\n",
    "                num_epochs: int,\n",
    "                optimizer_type,\n",
    "                loss_function,\n",
    "                learning_rate: float,\n",
    "                experiment: Experiment) -> None:\n",
    "    \"\"\"\n",
    "    runs one commplete training run, i.e. trains the model on your training data for\n",
    "    :param model: a pytorch model\n",
    "    :param train_data: a dataloader for getting the training instances\n",
    "    :param num_epochs: the number of epochs to train\n",
    "    :param optimizer_type: the type of optimizer to use for training\n",
    "    :param loss_function: the type of loss function to use\n",
    "    :param learning_rate: the learning rate for the optimizer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print(f'--------- Start Training ------------')\n",
    "\n",
    "    # Important: bring model into training mode\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optimizer_type(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # run training for specified number of epochs; use tqdm to keep track of progress / estimated run time \n",
    "    with experiment.train():\n",
    "        step=0\n",
    "\n",
    "        for epoch in tqdm(range(num_epochs), desc='Classifier Training\\n'):\n",
    "            # Cummulative loss per batch\n",
    "            cum_loss = 0\n",
    "            # Number of correct predictions\n",
    "            correct = 0\n",
    "            # Number of total tokens predicted\n",
    "            total = 0\n",
    "            \n",
    "            print(f'---------- Started Epoch {epoch} -----------')\n",
    "\n",
    "            for batch in tqdm(train_data):\n",
    "                # get the input instances \n",
    "                input_attributes = batch[0][0].to(device)\n",
    "                # get the corresponding labels\n",
    "                gold_labels = batch[0][1].to(device)\n",
    "                \n",
    "                # compute model predictions with current model parameters\n",
    "                model_output = model(input_attributes)\n",
    "    \n",
    "                # Compute Loss for current batch\n",
    "                loss = loss_function(model_output, gold_labels)\n",
    "                cum_loss += loss.item()\n",
    "                  \n",
    "                #Important: otherwise you add up your gradients for all batches and for all epochs\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                loss.backward()\n",
    "    \n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "            \n",
    "                ##################################################\n",
    "                # COMET\n",
    "                # Compute train accuracy\n",
    "                # Torch.max returns a namedtuple where of (value,indices)\n",
    "                # where ```values``` is the maximum value of each rou of the\n",
    "                # input tensor in the given dimension \n",
    "                _, predicted = torch.max(model_output.data, 1)\n",
    "\n",
    "                batch_total = gold_labels.size(0)\n",
    "                total += batch_total\n",
    "\n",
    "                batch_correct = (predicted == gold_labels.data).sum()\n",
    "                correct += batch_correct\n",
    "\n",
    "                 # Log batch_accuracy to Comet.ml; step is each batch\n",
    "            step+=1\n",
    "            experiment.log_metric(\"mean_epoch_accuracy\", correct / total, step=step)\n",
    "                ################################################\n",
    "                \n",
    "            mean_loss_per_epoch = cum_loss/len(train_data)\n",
    "            experiment.log_metric('Mean_loss_per_epoch',mean_loss_per_epoch,step)\n",
    "            print(mean_loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "HIDDEN_SIZE = 300\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "\n",
    "# Inputs\n",
    "EMBEDDINGS = pretrained_embeddings\n",
    "\n",
    "OPTIMIZER = optim.Adam\n",
    "\n",
    "LOSS_FUNCTION = nn.functional.cross_entropy\n",
    "\n",
    "EXPERIMENT = Experiment(project_name=\"LSTM-PosTagger - DevData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging the Hyper Parameters into Commet\n",
    "HyperParameters = {'HiddenSize': HIDDEN_SIZE,\n",
    "                   'NumEpochs': NUM_EPOCHS,\n",
    "                   'LearningRate': LEARNING_RATE\n",
    "                  }\n",
    "\n",
    "EXPERIMENT.log_parameters(HyperParameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TAGGER = LSTMPosTagger(pretrained_embeddings,HIDDEN_SIZE,len(tagset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model on Development Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_comet(POS_TAGGER,\n",
    "            dev_dataloader,\n",
    "            NUM_EPOCHS,\n",
    "            OPTIMIZER,\n",
    "            LOSS_FUNCTION,\n",
    "            LEARNING_RATE,\n",
    "            EXPERIMENT)\n",
    "\n",
    "torch.save(POS_TAGGER.state_dict(),'./LSTM_PosTagger_DEV - DevData5.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing accuracy on a given sentence\n",
    "\n",
    "The accuracy can only be computed on sentences we know the correct POS-Tags. Therefore we check the accuracy of the Development model on the first sentence provided by the development Dataloarder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Defined to compute accuracy\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    # iterate over each label and check\n",
    "    for true, predicted in zip(y_true, y_pred):\n",
    "        if true == predicted:\n",
    "            correct_predictions += 1\n",
    "    # compute the accuracy\n",
    "    accuracy = correct_predictions/len(y_true)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMPosTagger(pretrained_embeddings,300,len(tagset))\n",
    "model.load_state_dict(torch.load('./LSTM_PosTagger_DEV - DevData1.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dev_dataloader:\n",
    "    input_attributes = batch[0][0].to(device)\n",
    "    gold_labels = batch[0][1].to(device)\n",
    "    \n",
    "    model_output = model(input_attributes)\n",
    "    \n",
    "    _, predicted = torch.max(model_output.data, 1)\n",
    "    \n",
    "    print('Tagging Accuracy: {}'.format(compute_accuracy(gold_labels, predicted)))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Dev Models Accuracy on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(path, Data, Model):\n",
    "    Model.load_state_dict(torch.load(path))\n",
    "    Model.eval()\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        correct=0\n",
    "        total=0\n",
    "           \n",
    "        test_predictions = None\n",
    "        test_targets = None\n",
    "        \n",
    "        for batch in Data:\n",
    "            input_attributes = batch[0][0].to(device)\n",
    "            # get the corresponding labels\n",
    "            gold_labels = batch[0][1].to(device)\n",
    "            model_output = Model(input_attributes)\n",
    "            _, predicted = torch.max(model_output.data, 1)\n",
    "            \n",
    "            \n",
    "            batch_total = gold_labels.size(0)\n",
    "            total += batch_total\n",
    "    \n",
    "            batch_correct = (predicted == gold_labels.data).sum()\n",
    "            correct += batch_correct\n",
    "            \n",
    "    model_accurracy = correct/total        \n",
    "    print('model accuracy: {:.4f}'.format(model_accurracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './LSTM_PosTagger_DEV - DevData1.pt'\n",
    "lstm = LSTMPosTagger(pretrained_embeddings,300,len(tagset))\n",
    "\n",
    "eval_model(path, test_dataloader,lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './LSTM_PosTagger_DEV - DevData2.pt'\n",
    "lstm = LSTMPosTagger(pretrained_embeddings,300,len(tagset))\n",
    "\n",
    "eval_model(path, test_dataloader,lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './LSTM_PosTagger_DEV - DevData3.pt'\n",
    "lstm = LSTMPosTagger(pretrained_embeddings,300,len(tagset))\n",
    "\n",
    "eval_model(path, test_dataloader,lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './LSTM_PosTagger_DEV - DevData4.pt'\n",
    "lstm = LSTMPosTagger(pretrained_embeddings,300,len(tagset),False)\n",
    "\n",
    "eval_model(path, test_dataloader,lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Train model on training data\n",
    " \n",
    " The model will be trained using the hyperparameters that resulted on the best accuracy for the model trained on the development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 300\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Inputs\n",
    "EMBEDDINGS = pretrained_embeddings\n",
    "\n",
    "OPTIMIZER = optim.Adam\n",
    "\n",
    "LOSS_FUNCTION = nn.functional.cross_entropy\n",
    "\n",
    "EXPERIMENT = Experiment(project_name=\"LSTM-PosTagger - Training Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging the Hyper Parameters into Commet\n",
    "HyperParameters = {'HiddenSize': HIDDEN_SIZE,\n",
    "                   'NumEpochs': NUM_EPOCHS,\n",
    "                   'LearningRate': LEARNING_RATE\n",
    "                  }\n",
    "\n",
    "EXPERIMENT.log_parameters(HyperParameters)\n",
    "\n",
    "# Initializing the model\n",
    "POS_TAGGER = LSTMPosTagger(pretrained_embeddings,HIDDEN_SIZE,len(tagset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_comet(POS_TAGGER,\n",
    "            train_dataloader,\n",
    "            NUM_EPOCHS,\n",
    "            OPTIMIZER,\n",
    "            LOSS_FUNCTION,\n",
    "            LEARNING_RATE,\n",
    "            EXPERIMENT)\n",
    "\n",
    "torch.save(POS_TAGGER.state_dict(),'./LSTM_PosTagger_Train - TrainData1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './LSTM_PosTagger_Train - TrainData1.pt'\n",
    "lstm = LSTMPosTagger(pretrained_embeddings,300,len(tagset))\n",
    "\n",
    "eval_model(path, test_dataloader,lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
