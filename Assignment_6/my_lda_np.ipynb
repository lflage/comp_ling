{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While reading the corpus we assign a unique id to each word and append it to a list, this list\n",
    "# corresponds to the document, and we append it to another list which corresponds to our corpus.\n",
    "\n",
    "corpus_path = '.movies-pp.txt'\n",
    "with open(corpus_path, 'r') as file:\n",
    "    docs = file.readlines()[1:]\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    n_docs = []\n",
    "    id_word = 0\n",
    "    \n",
    "    for doc in docs:\n",
    "        current_doc = []\n",
    "        \n",
    "        for word in doc.split():\n",
    "            #print(word)\n",
    "            if word in word_to_id:\n",
    "                current_doc.append(word_to_id[word])\n",
    "            else:\n",
    "                current_doc.append(id_word)\n",
    "                word_to_id[word] = id_word\n",
    "                id_to_word[id_word] = word\n",
    "                id_word +=1\n",
    "        n_docs.append(current_doc)\n",
    "\n",
    "# Transforming the documents into a list of word ids will make it easier to access the word\n",
    "# and it's topic assignment in Z\n",
    "    \n",
    "vocab = set([word for doc in docs for word in doc.split()])\n",
    "l_vocab = len(vocab)\n",
    "doc_n = len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Parameters\n",
    "\n",
    "alpha = 0.02\n",
    "beta = 0.1\n",
    "iter_n = 500\n",
    "topic_n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix with dimension W x T, contains the number of times the word w is assigned to topic t\n",
    "C_wt = np.zeros([l_vocab, topic_n]) \n",
    "\n",
    "# Matrix with dimentions D x T, contains the number times a topic t appears in document d\n",
    "C_dt = np.zeros([doc_n, topic_n])\n",
    "\n",
    "# column vector N_z contains the number of times a topic occurs\n",
    "N_z = np.zeros([topic_n])\n",
    "\n",
    "# Z is the topic assignments for all tokens. Each key corresponds to a document, its value is \n",
    "# a list of topic assignments for the document key.\n",
    "Z = {}\n",
    "\n",
    "# A dict with the documents length. It is initialized with values to be used in the sampling phase.\n",
    "# Its the denominator of the theta element of the Gibbs sampling formula\n",
    "doc_size = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random initialization of topics\n",
    "\n",
    "for i_doc, doc in tqdm(enumerate(n_docs)):\n",
    "    current_doc = []\n",
    "    for word in doc:\n",
    "        # Generate a random topic\n",
    "        k = rd.randrange(topic_n)\n",
    "        \n",
    "        # Appending the topic to the current doc\n",
    "        current_doc.append(k)\n",
    "        \n",
    "        # Incrementing the count of topic occurance\n",
    "        N_z[k] += 1\n",
    "        \n",
    "        # Incrementing the count of word with topic\n",
    "        C_wt[word, k] += 1\n",
    "        \n",
    "        # Incrementing the count of topic in document\n",
    "        C_dt[i_doc, k] += 1\n",
    "        \n",
    "    # appending the doc to dict Z\n",
    "    Z[i_doc] = current_doc\n",
    "    # appending the theta denominator\n",
    "    doc_size[i_doc] = (len(current_doc)-1)+topic_n*alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing the constants to the matrices\n",
    "\n",
    "C_wt = C_wt + beta\n",
    "C_dt = C_dt + alpha\n",
    "N_z = N_z + l_vocab * beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(np.arange(iter_n)):\n",
    "    for i_doc, doc in tqdm(enumerate(n_docs)):\n",
    "        current_doc_size = doc_size[i_doc]\n",
    "        \n",
    "        for i_word, word in enumerate(doc):\n",
    "            assigned_topic = Z[i_doc][i_word]\n",
    "            # Subtracting counts\n",
    "            C_wt[word, assigned_topic] -= 1\n",
    "            C_dt[i_doc, assigned_topic] -= 1\n",
    "            N_z[assigned_topic] -= 1\n",
    "        \n",
    "            # Calculating probability distribution\n",
    "            top = C_wt[word,:]*C_dt[i_doc,:]\n",
    "            botton = N_z * current_doc_size\n",
    "            p = top/botton\n",
    "            # The new topic is sampled from an np.array with range topic_n,\n",
    "            # which every position of this array receives a probability p to be sampled.\n",
    "            new_topic = np.random.choice(a=topic_n,p=p/p.sum())\n",
    "           \n",
    "            # updating the new counts\n",
    "            Z[i_doc][i_word] = new_topic\n",
    "            C_wt[word, new_topic] += 1\n",
    "            C_dt[i_doc,new_topic] += 1\n",
    "            N_z[new_topic] += 1                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the resulting matrix\n",
    "save_path = './C_wt.npy'\n",
    "np.save(save_path, C_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing saved result. Set the if statement to True depending on what to do.\n",
    "load_path = './C_wt_iter50.npy'\n",
    "if 1==0:\n",
    "    with open(load_path,'rb') as file:\n",
    "        temp_wt = np.load(file)\n",
    "if 0==0:\n",
    "    temp_wt = C_wt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finding the most frequent words\n",
    "\n",
    "most_freq_words = []\n",
    "for topic in range(topic_n):\n",
    "    # argsort gives us the indexes of the sorted elements in ascending order.\n",
    "    # To get the highest numbers we multiply the matrix by -1, this way the sm\n",
    "    most_freq_ids = (-temp_wt[:,topic]).argsort()[:15]\n",
    "    # print(most_freq_ids)\n",
    "    c_most_freq_words = []\n",
    "    for ix in most_freq_ids:\n",
    "        c_most_freq_words.append(id_to_word[ix])\n",
    "    most_freq_words.append(c_most_freq_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(most_freq_words).T\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "This discussion is based on a partial result ran with only 50 iterations.\n",
    "\n",
    "In general it was found that actors, directors and the movies they made are grouped together. For group 8 we see 'schumacher' a movie director, 'jack', which can be from actor \"jack nicholson\" and 'batman' grouped together.\n",
    "\n",
    "The easiest to see a smantic grouping is group 4, wich seems to have bundled animations for children and disney like movies together.\n",
    "\n",
    "In group 9 we see the tokens '2' and 'sequel' appearing together with 'horror', 'scream', 'killer', which in my opinion suggests that horror movies have too many sequels.\n",
    "\n",
    "In group 17 we see also that actors and movies were grouped together, as we have 'truman','show','jim','carrey' together. Also interesting to notice how 'political','science' appeared together, as this movie is an interesting commentary on reality shows and vigilance states."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
